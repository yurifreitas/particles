import os
import dask.dataframe as dd
import pandas as pd
import numpy as np
import json
from sklearn.preprocessing import StandardScaler
import umap
from hdbscan import HDBSCAN
import networkx as nx

# Diret√≥rio para salvar os arquivos processados
PROCESSED_PARQUET_DIR = "/app/data/processed_parquet_parts"
CHECKPOINT_FILE = "/app/logs/processing_checkpoint.json"

# Criar diret√≥rio se n√£o existir
os.makedirs(PROCESSED_PARQUET_DIR, exist_ok=True)
def is_valid_root_file(filepath):
    """Verifica se o arquivo ROOT √© v√°lido antes de abrir com uproot."""
    try:
        if not os.path.exists(filepath):
            print(f"Erro: O arquivo {filepath} n√£o existe.")
            return False
        if os.path.getsize(filepath) == 0:
            print(f"Erro: O arquivo {filepath} est√° vazio.")
            return False

        import uproot
        with uproot.open(filepath) as file:
            return True  # Se abrir sem erro, √© v√°lido
    except Exception as e:
        print(f"Aviso: O arquivo {filepath} n√£o √© v√°lido. Erro: {e}")
        return False
def load_checkpoint():
    """Carrega o progresso do processamento para evitar reprocessamento."""
    if os.path.exists(CHECKPOINT_FILE):
        with open(CHECKPOINT_FILE, "r") as f:
            return json.load(f)
    return {}

def save_checkpoint(checkpoint):
    """Salva o progresso do processamento."""
    with open(CHECKPOINT_FILE, "w") as f:
        json.dump(checkpoint, f, indent=4)

def generate_fractal_connections(n=1000, depth=3):
    """
    Gera conex√µes fractais complexas simulando hiperdimensionalidade.

    :param n: N√∫mero de pontos.
    :param depth: Profundidade da estrutura fractal.
    :return: DataFrame representando as conex√µes.
    """
    G = nx.Graph()

    for i in range(n):
        G.add_node(i)
        if i > 0:
            parent = i // (2 + np.random.randint(0, 2))
            G.add_edge(i, parent)

    for _ in range(depth):
        edges = list(G.edges())
        for edge in edges:
            a, b = edge
            mid = len(G.nodes)
            G.add_node(mid)
            G.add_edge(a, mid)
            G.add_edge(mid, b)
            G.remove_edge(a, b)

    # Criando estrutura para armazenar no Parquet
    fractal_data = {
        "node": [],
        "parent": []
    }
    for edge in G.edges():
        fractal_data["node"].append(edge[0])
        fractal_data["parent"].append(edge[1])

    return pd.DataFrame(fractal_data)

def process_parquet_file(input_file, checkpoint):
    """
    Processa um √∫nico arquivo Parquet e salva de forma incremental.

    :param input_file: Caminho do arquivo Parquet original.
    :param checkpoint: Dicion√°rio de checkpoint.
    """
    file_name = os.path.basename(input_file)
    output_file = os.path.join(PROCESSED_PARQUET_DIR, file_name)

    if file_name in checkpoint:
        print(f"‚úÖ J√° processado: {file_name}, pulando...")
        return

    print(f"üìÇ Processando: {file_name}")

    df = dd.read_parquet(input_file).compute()

    # Aplicar UMAP para redu√ß√£o de dimensionalidade
    if 'U1' not in df.columns or 'U2' not in df.columns or 'U3' not in df.columns:
        print(f"‚ö†Ô∏è Aplicando UMAP para redu√ß√£o de dimensionalidade...")
        umap_reducer = umap.UMAP(n_neighbors=50, min_dist=0.02, n_components=3, random_state=None)
        df[['U1', 'U2', 'U3']] = umap_reducer.fit_transform(df[['MuonsAuxDyn.pt', 'MuonsAuxDyn.eta', 'MuonsAuxDyn.phi']])
        print(f"‚úÖ UMAP conclu√≠do.")

    # Aplicar HDBSCAN para clustering
    if 'cluster' not in df.columns:
        print(f"‚ö†Ô∏è Aplicando HDBSCAN para clustering...")
        scaler = StandardScaler()
        df_scaled = scaler.fit_transform(df[['MuonsAuxDyn.pt', 'MuonsAuxDyn.eta', 'MuonsAuxDyn.phi']])
        clusterer = HDBSCAN(min_cluster_size=10)
        df['cluster'] = clusterer.fit_predict(df_scaled)
        print(f"‚úÖ Clustering conclu√≠do.")

    # Adicionando conex√µes fractais
    fractal_df = generate_fractal_connections(n=len(df), depth=3)
    df = pd.concat([df.reset_index(drop=True), fractal_df.reset_index(drop=True)], axis=1)

    # Salvando o arquivo processado
    df.to_parquet(output_file, index=False)
    print(f"‚úÖ Arquivo salvo: {output_file}")

    # Atualizar checkpoint
    checkpoint[file_name] = True
    save_checkpoint(checkpoint)

def process_all_parquet_files(input_dir):
    """
    Processa todos os arquivos Parquet e os salva em partes para evitar estouro de mem√≥ria.

    :param input_dir: Diret√≥rio contendo arquivos Parquet brutos.
    """
    checkpoint = load_checkpoint()

    for filename in sorted(os.listdir(input_dir)):
        if filename.endswith(".parquet"):
            process_parquet_file(os.path.join(input_dir, filename), checkpoint)

if __name__ == "__main__":
    input_parquet_dir = "/app/data/parquet/"
    process_all_parquet_files(input_parquet_dir)
